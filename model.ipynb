{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.687299Z",
     "start_time": "2025-01-17T12:26:31.685482Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List\n",
    "\n",
    "class RepresentationModuleEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 snp_dim: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        in_dim = snp_dim\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, h_dim),\n",
    "                    nn.ELU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "\n",
    "        modules.append(nn.Linear(hidden_dims[-1], latent_dim * 2))\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        result = self.encoder(x)\n",
    "\n",
    "        mu = result[:, :self.latent_dim]\n",
    "        log_var = result[:, self.latent_dim:]\n",
    "\n",
    "        return mu, log_var\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return mu, log_var, z\n",
    "\n",
    "\n",
    "\n",
    "class RepresentationModuleDecoder(nn.Module):\n",
    "    def __init__(self,snp_dim: int, latent_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "\n",
    "                # Build Decoder\n",
    "        modules = []\n",
    "\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        hidden_dims = hidden_dims[::-1]\n",
    "        in_dim = latent_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(in_dim, h_dim),\n",
    "                    nn.ELU())\n",
    "            )\n",
    "            in_dim = h_dim\n",
    "\n",
    "        modules.append(nn.Linear(hidden_dims[-1], snp_dim))\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "    def decode(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        result = self.decoder(z)\n",
    "        return result\n",
    "\n",
    "    def forward(self, z: torch.Tensor):\n",
    "        return self.decode(z)\n"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "cell_type": "code",
   "source": [
    "class AssociationModuleGenerator(nn.Module):\n",
    "    def __init__(self, input_dim: int, generator_hidden_dims: list[int], image_dim: int):\n",
    "        super().__init__()\n",
    "        # AssociationModule is similar to a GAN consisting of a generator and a discriminator\n",
    "        # The generator generates from the latent space consisting of the output from the representation module concatenated with a demographic vector\n",
    "        # It then outputs a fake image vector xmri and an attentive mask a\n",
    "        # the discriminator takes the fake image vector and the real image vector and outputs a probability of the image being real\n",
    "\n",
    "        # The generator is a simple feedforward network with the latent space concatenated with the demographic vector\n",
    "        # The discriminator is a simple feedforward network with the image vector as input\n",
    "\n",
    "        generator_modules = []\n",
    "        if generator_hidden_dims is None:\n",
    "            generator_hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        current_dim = input_dim\n",
    "        for h_dim in generator_hidden_dims:\n",
    "            generator_modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(current_dim, h_dim),\n",
    "                    nn.ELU())\n",
    "            )\n",
    "            current_dim = h_dim\n",
    "\n",
    "        generator_modules.append(nn.Sequential(nn.Linear(generator_hidden_dims[-1], image_dim * 2), nn.Sigmoid()))\n",
    "\n",
    "        self.generator = nn.Sequential(*generator_modules)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, demographic: torch.Tensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # concatenate the demographic vector with the latent space\n",
    "        x = torch.cat((x, demographic), dim=1)\n",
    "        generator_output = self.generator(x)\n",
    "        # split the output into the fake image vector and the attentive mask by splitting the output in half\n",
    "        fake_image = generator_output[:, :generator_output.shape[1] // 2]\n",
    "        attentive_mask = generator_output[:, generator_output.shape[1] // 2:]\n",
    "        return fake_image, attentive_mask\n",
    "\n",
    "class AssociationModuleDiscriminator(nn.Module):\n",
    "    def __init__(self, image_dim: int):\n",
    "        super().__init__()\n",
    "        self.discriminator = nn.Sequential(\n",
    "            nn.Linear(image_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return self.discriminator(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.702039Z",
     "start_time": "2025-01-17T12:26:31.693920Z"
    }
   },
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.702199Z",
     "start_time": "2025-01-17T12:26:31.699610Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiagnosticianModule (nn.Module):\n",
    "    def __init__(self, input_dim: int, reduction_dim: int, classification_targets: int):\n",
    "        super().__init__()\n",
    "        # perform regression and classification using two linear layers\n",
    "        # after reducing dimensionality to\n",
    "        self.dim_reduction = nn.Sequential(\n",
    "            nn.Linear(input_dim, reduction_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(reduction_dim, classification_targets)\n",
    "        self.regressor = nn.Linear(reduction_dim, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, apply_logistic_activation: bool, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        reduced_dims = self.dim_reduction(x)\n",
    "        classification_output = self.classifier(reduced_dims)\n",
    "        regression_output = self.regressor(reduced_dims)\n",
    "        if apply_logistic_activation:\n",
    "            y_hat = nn.functional.softmax(classification_output)\n",
    "            s_hat = nn.functional.sigmoid(regression_output)\n",
    "            return y_hat, s_hat\n",
    "        return classification_output, regression_output"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.710648Z",
     "start_time": "2025-01-17T12:26:31.704922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GenerativeDiscriminativeModel(nn.Module):\n",
    "    def __init__(self, snp_dims: int, mri_dims: int, demographic_dims, classification_dims: int):\n",
    "        super().__init__()\n",
    "        self.encoder = RepresentationModuleEncoder(snp_dims, 50, [500])\n",
    "        self.decoder = RepresentationModuleDecoder(snp_dims, 50, [500])\n",
    "\n",
    "        self.generator = AssociationModuleGenerator(50 + demographic_dims, [100], mri_dims)\n",
    "        self.discriminator = AssociationModuleDiscriminator(mri_dims)\n",
    "\n",
    "        self.diagnostician = DiagnosticianModule(mri_dims, 25, classification_dims)\n",
    "\n",
    "    def forward(self, snp_features: torch.Tensor, mri_features: torch.Tensor, demographic_features: torch.Tensor):\n",
    "        mu, log_var, z = self.encoder(snp_features)\n",
    "        snp_reconstruction = self.decoder(z)\n",
    "\n",
    "        # concatenate z and demographic features\n",
    "        xmri_fake, attention_mask = self.generator(z, demographic_features)\n",
    "        xmri_real = mri_features\n",
    "        discriminator_output_fake = self.discriminator(xmri_fake)\n",
    "        discriminator_output_real = self.discriminator(xmri_real)\n",
    "\n",
    "        # hadamard product between the attention mask and the real image\n",
    "        attended_mri_features = attention_mask * xmri_real\n",
    "\n",
    "        y_logits, mmsr_regression = self.diagnostician(attended_mri_features, False)\n",
    "\n",
    "        return snp_reconstruction, mu, log_var, discriminator_output_fake, discriminator_output_real, y_logits, mmsr_regression"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.715669Z",
     "start_time": "2025-01-17T12:26:31.714110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class AdniDataset(Dataset):\n",
    "    def __init__(self, snp_data: np.ndarray, mri_data: np.ndarray, demographic_data: np.ndarray, diagnosis_data: np.ndarray):\n",
    "        self.raw_snp_data = np.copy(snp_data)\n",
    "        self.mri_data = torch.from_numpy(np.copy(mri_data)).to(dtype=torch.float32)\n",
    "        self.demographic_data = torch.from_numpy(np.copy(demographic_data)).to(dtype=torch.float32)\n",
    "        diagnosis_data = np.copy(diagnosis_data)\n",
    "        self.mmse_data = torch.from_numpy(diagnosis_data[:, 1]).to(dtype=torch.float32)\n",
    "        self.diagnosis_data = torch.from_numpy(diagnosis_data[:, 0]).to(dtype=torch.float32)\n",
    "        self.snp_data = torch.zeros((self.raw_snp_data.shape[0], self.raw_snp_data.shape[1]))\n",
    "\n",
    "\n",
    "    def normalize(self, normalization_matrix: np.ndarray | None = None) -> tuple[np.ndarray, np.ndarray]:\n",
    "        # we have to normalize snp data by computing a normalization matrix. We want as rows all possible values and as columns probability of that value in the dataset\n",
    "        # we then use this matrix to normalize the snp data\n",
    "\n",
    "        # get all unique values in the snp data\n",
    "        if normalization_matrix is None:\n",
    "            unique_values = np.unique(self.raw_snp_data)\n",
    "            normalization_matrix = np.zeros((len(unique_values), self.raw_snp_data.shape[1]))\n",
    "            for i, value in enumerate(unique_values):\n",
    "                normalization_matrix[i] = (self.raw_snp_data == value).sum(axis=0) / self.raw_snp_data.shape[0]\n",
    "\n",
    "        normalized_snp_data = np.zeros((self.raw_snp_data.shape[0], self.raw_snp_data.shape[1]))\n",
    "        # we now have a matrix where each row is a unique value and each column is the probability of that value in the dataset\n",
    "        # we can now normalize the snp data by replacing each value with the corresponding row in the normalization matrix\n",
    "        for i in range(self.raw_snp_data.shape[0]):\n",
    "            for j in range(self.raw_snp_data.shape[1]):\n",
    "                normalized_snp_data[i, j] = normalization_matrix[self.raw_snp_data[i, j], j]\n",
    "\n",
    "        self.snp_data = torch.from_numpy(normalized_snp_data).to(dtype=torch.float32)\n",
    "        return normalized_snp_data, normalization_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.snp_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.snp_data[idx], self.mri_data[idx], self.demographic_data[idx], self.diagnosis_data[idx], self.mmse_data[idx]\n"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.719195Z",
     "start_time": "2025-01-17T12:26:31.717894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as  np\n",
    "#dataset_base_path = \"/media/jfallmann/T9/University/master_thesis/dataset\"\n",
    "dataset_base_path = \"/Volumes/T9/University/master_thesis/dataset\"\n",
    "\n",
    "mri_raw_path = f\"{dataset_base_path}/mri/raw\"\n",
    "mri_base_path = f\"{dataset_base_path}/mri\"\n",
    "snp_raw_path = f\"{dataset_base_path}/snp/raw\"\n",
    "mri_bids_path = f\"{dataset_base_path}/mri/bids\"\n",
    "mri_fastsurfer_out = f\"{dataset_base_path}/mri/processed\"\n",
    "tables_path = f\"{dataset_base_path}/tables\""
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.945528Z",
     "start_time": "2025-01-17T12:26:31.721535Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import mri data\n",
    "mri_data = np.load(f\"{mri_base_path}/processed_volumes.npy\")\n",
    "# import snp data\n",
    "snp_data = np.load(f\"{dataset_base_path}/snp/processed/genomes.npy\")\n",
    "# import demographic data\n",
    "demographic_data = np.load(f\"{dataset_base_path}/tables/demographic_data.npy\")\n",
    "# import diagnosis data\n",
    "diagnosis_data = np.load(f\"{dataset_base_path}/tables/diagnosis_data.npy\")\n",
    "\n",
    "zero_rows = np.where(~mri_data.any(axis=1))[0]\n",
    "mri_data = np.delete(mri_data, zero_rows, axis=0)\n",
    "snp_data = np.delete(snp_data, zero_rows, axis=0)\n",
    "demographic_data = np.delete(demographic_data, zero_rows, axis=0)\n",
    "diagnosis_data = np.delete(diagnosis_data, zero_rows, axis=0)"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.950047Z",
     "start_time": "2025-01-17T12:26:31.948691Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_dataloader(classification_mode: str, batch_size, split, full_snp_data,full_mri_data, full_demographic_data, full_diagnosis_data):\n",
    "    Y = diagnosis_data[:, 0]\n",
    "    rows = None\n",
    "    if classification_mode == \"cn/ad\":\n",
    "        rows = np.where((Y == 1) | (Y == 3))\n",
    "\n",
    "    c_snp_data = full_snp_data[rows]\n",
    "    c_mri_data = full_mri_data[rows]\n",
    "    c_demographic_data = full_demographic_data[rows]\n",
    "    c_diagnosis_data = full_diagnosis_data[rows]\n",
    "\n",
    "    diagnosis = c_diagnosis_data[:, 0]\n",
    "    diagnosis = (diagnosis - np.min(diagnosis)) / (np.max(diagnosis) - np.min(diagnosis))\n",
    "    c_diagnosis_data[:, 0] = diagnosis\n",
    "\n",
    "    snp_train, snp_test, mri_train, mri_test, demographic_train, demographic_test, diagnosis_train, diagnosis_test = train_test_split(c_snp_data, c_mri_data, c_demographic_data, c_diagnosis_data, test_size=split, random_state=42)\n",
    "\n",
    "    train_dataset = AdniDataset(snp_train, mri_train, demographic_train, diagnosis_train)\n",
    "    _, normalization_matrix = train_dataset.normalize()\n",
    "\n",
    "    test_dataset = AdniDataset(snp_test, mri_test, demographic_test, diagnosis_test)\n",
    "    _, _ = test_dataset.normalize(normalization_matrix)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ],
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:26:31.953363Z",
     "start_time": "2025-01-17T12:26:31.951885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def elbo(reconstruction, input, mu, log_var, kld_weight) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    recons_loss =F.mse_loss(reconstruction, input)\n",
    "    kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "    loss = recons_loss + kld_weight * kld_loss\n",
    "    return loss,recons_loss.detach(),-kld_loss.detach()"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:31:07.256880Z",
     "start_time": "2025-01-17T12:31:07.255603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "def train(model, device,train_loader, optimizer, num_epochs):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    generator_loss_function = nn.MSELoss().to(device)\n",
    "    discriminator_loss_function = nn.MSELoss().to(device)\n",
    "    classification_loss_function = nn.BCEWithLogitsLoss().to(device)\n",
    "    regression_loss_function = nn.MSELoss().to(device)\n",
    "    scheduler = ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        elbo_losses = []\n",
    "        generator_losses = []\n",
    "        discriminator_losses = []\n",
    "        classification_losses = []\n",
    "        regression_losses = []\n",
    "\n",
    "        for snp, mri, demographic, diagnosis, mmse in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            snp = snp.to(device)\n",
    "            mri = mri.to(device)\n",
    "            demographic = demographic.to(device)\n",
    "            diagnosis = diagnosis.unsqueeze(1).to(device)\n",
    "            mmse = diagnosis.to(device)\n",
    "            snp_reconstruction, mu, log_var, discriminator_output_fake, discriminator_output_real, y_logits, mmsr_regression = model(snp, mri, demographic)\n",
    "\n",
    "            # calculate losses\n",
    "            elbo_loss, recon_loss, kld_loss = elbo(snp_reconstruction, snp, mu, log_var, 0.1)\n",
    "            generator_loss = generator_loss_function(discriminator_output_fake, torch.ones_like(discriminator_output_fake))\n",
    "            discriminator_loss = discriminator_loss_function(discriminator_output_real, torch.ones_like(discriminator_output_real)) + discriminator_loss_function(discriminator_output_fake, torch.zeros_like(discriminator_output_fake))\n",
    "            classification_loss = classification_loss_function(y_logits, diagnosis)\n",
    "            regression_loss = regression_loss_function(mmsr_regression, mmse)\n",
    "\n",
    "            loss = (0.7*elbo_loss + 0.5*generator_loss + discriminator_loss + classification_loss + 0.7*regression_loss)/ 3.9\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            elbo_losses.append(elbo_loss.item())\n",
    "            generator_losses.append(generator_loss.item())\n",
    "            discriminator_losses.append(discriminator_loss.item())\n",
    "            classification_losses.append(classification_loss.item())\n",
    "            regression_losses.append(regression_loss.item())\n",
    "        train_losses.append(total_loss)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            scheduler.step()\n",
    "            print(f\"Epoch {epoch} loss: {total_loss}. Elbo loss: {np.mean(elbo_losses)}. Generator loss: {np.mean(generator_losses)}. Discriminator loss: {np.mean(discriminator_losses)}. Classification loss: {np.mean(classification_losses)}. Regression loss: {np.mean(regression_losses)}\")\n",
    "    return train_losses"
   ],
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T12:35:13.210176Z",
     "start_time": "2025-01-17T12:31:09.053549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create model\n",
    "diagnosis = [1, 3]\n",
    "model = GenerativeDiscriminativeModel(snp_data.shape[1], mri_data.shape[1], demographic_data.shape[1], 1)\n",
    "num_epochs = 2000\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "batch_size = 5\n",
    "device = 'cpu'\n",
    "\n",
    "train_loader, test_loader = get_dataloader(\"cn/ad\", batch_size, 0.2, snp_data, mri_data, demographic_data, diagnosis_data)\n",
    "\n",
    "train_losses = train(model,device, train_loader, optimizer, num_epochs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 14.385527670383453. Elbo loss: 0.5104268496074984. Generator loss: 0.4237694634545234. Discriminator loss: 0.39360825188698306. Classification loss: 0.6943013937242569. Regression loss: 0.21814157259500316\n",
      "Epoch 10 loss: 9.742264851927757. Elbo loss: 0.041055087240472916. Generator loss: 0.44343765320316436. Discriminator loss: 0.28914570327728023. Classification loss: 0.5534464466956354. Regression loss: 0.18941468721435917\n",
      "Epoch 20 loss: 9.383879274129868. Elbo loss: 0.084534632342477. Generator loss: 0.44414996620147457. Discriminator loss: 0.229658686826306. Classification loss: 0.5422667245711049. Regression loss: 0.18196848876053287\n",
      "Epoch 30 loss: 9.167731329798698. Elbo loss: 0.16243517350765965. Generator loss: 0.44374262517498386. Discriminator loss: 0.19313270766888896. Classification loss: 0.5127860355761743. Regression loss: 0.1598072913625548\n",
      "Epoch 40 loss: 8.458666801452637. Elbo loss: 0.177872953636031. Generator loss: 0.4453290306752728. Discriminator loss: 0.1693107682851053. Classification loss: 0.4552550734050812. Regression loss: 0.13201922962381954\n",
      "Epoch 50 loss: 8.007697850465775. Elbo loss: 0.26380250626994717. Generator loss: 0.44432466549258076. Discriminator loss: 0.15500755079330936. Classification loss: 0.3671420178105754. Regression loss: 0.11206632529595686\n",
      "Epoch 60 loss: 7.23945052921772. Elbo loss: 0.27906533450849597. Generator loss: 0.44607000581679807. Discriminator loss: 0.14421804874174057. Classification loss: 0.29196078991217. Regression loss: 0.080300037561345\n",
      "Epoch 70 loss: 5.501688003540039. Elbo loss: 0.22318289212642178. Generator loss: 0.4445079122820208. Discriminator loss: 0.13765915314997396. Classification loss: 0.15376666837161587. Regression loss: 0.031771531835528874\n",
      "Epoch 80 loss: 5.331479303538799. Elbo loss: 0.2630607860703622. Generator loss: 0.4424076762891585. Discriminator loss: 0.1336983074584315. Classification loss: 0.11235899075625404. Regression loss: 0.027615469302080812\n",
      "Epoch 90 loss: 4.409464538097382. Elbo loss: 0.16306073579096025. Generator loss: 0.4444010180811728. Discriminator loss: 0.12873096043063748. Classification loss: 0.07495929997774863. Regression loss: 0.021008315862291644\n",
      "Epoch 100 loss: 5.087253287434578. Elbo loss: 0.24839000716324774. Generator loss: 0.4439258373552753. Discriminator loss: 0.12596186154311703. Classification loss: 0.09835164534348634. Regression loss: 0.02837124411504896\n",
      "Epoch 110 loss: 4.443773254752159. Elbo loss: 0.22367638445669605. Generator loss: 0.4440302791134004. Discriminator loss: 0.12368306517601013. Classification loss: 0.044850252508636446. Regression loss: 0.017047784584123763\n",
      "Epoch 120 loss: 4.593104064464569. Elbo loss: 0.17352546006441116. Generator loss: 0.445065951155078. Discriminator loss: 0.1212330789815995. Classification loss: 0.09394169911261528. Regression loss: 0.02666652523874936\n",
      "Epoch 130 loss: 4.567859172821045. Elbo loss: 0.21735601583796163. Generator loss: 0.4442235096808403. Discriminator loss: 0.12019719760264119. Classification loss: 0.0655641298320505. Regression loss: 0.02091983256621226\n",
      "Epoch 140 loss: 4.400112457573414. Elbo loss: 0.2169035632283457. Generator loss: 0.444584644609882. Discriminator loss: 0.11880700073895915. Classification loss: 0.04883112895092176. Regression loss: 0.01685656937620332\n",
      "Epoch 150 loss: 4.297005318105221. Elbo loss: 0.18857592849000807. Generator loss: 0.44460183766580397. Discriminator loss: 0.11778368512468954. Classification loss: 0.05536477408942676. Regression loss: 0.018769264893396007\n",
      "Epoch 160 loss: 3.799797862768173. Elbo loss: 0.1671837818238043. Generator loss: 0.44454310209520403. Discriminator loss: 0.11698800854144557. Classification loss: 0.014916061093249628. Regression loss: 0.009764041101950552\n",
      "Epoch 170 loss: 4.7932546734809875. Elbo loss: 0.26762124511503405. Generator loss: 0.4447581883399717. Discriminator loss: 0.11615274774451409. Classification loss: 0.05974481487646699. Regression loss: 0.024872647417190995\n",
      "Epoch 180 loss: 5.499184474349022. Elbo loss: 0.3397507297415887. Generator loss: 0.4445178143439754. Discriminator loss: 0.11569010225995895. Classification loss: 0.0944316754523184. Regression loss: 0.03089530087562443\n",
      "Epoch 190 loss: 3.8937122225761414. Elbo loss: 0.19116385905973374. Generator loss: 0.4439888615762034. Discriminator loss: 0.11541654121491217. Classification loss: 0.011993308487768856. Regression loss: 0.009478793543342861\n",
      "Epoch 200 loss: 4.164480626583099. Elbo loss: 0.24833843352333193. Generator loss: 0.44462643611815666. Discriminator loss: 0.11466631798013564. Classification loss: 0.007950621762222821. Regression loss: 0.0073592584282572895\n",
      "Epoch 210 loss: 3.509039618074894. Elbo loss: 0.12922971911968723. Generator loss: 0.44439376650318024. Discriminator loss: 0.11437457295194749. Classification loss: 0.008457514273393299. Regression loss: 0.008528654877145233\n",
      "Epoch 220 loss: 3.7225529700517654. Elbo loss: 0.1637939342087315. Generator loss: 0.444378549052823. Discriminator loss: 0.11402965144764993. Classification loss: 0.009962890113163139. Regression loss: 0.010690891535036386\n",
      "Epoch 230 loss: 3.5808854699134827. Elbo loss: 0.1410638286221412. Generator loss: 0.4447007458056173. Discriminator loss: 0.11358457274975316. Classification loss: 0.009115327213470253. Regression loss: 0.00957651260062947\n",
      "Epoch 240 loss: 4.080951474606991. Elbo loss: 0.19231769490626552. Generator loss: 0.4441791349841702. Discriminator loss: 0.1135920716389533. Classification loss: 0.03284035513230089. Regression loss: 0.014665239139445006\n",
      "Epoch 250 loss: 4.615214511752129. Elbo loss: 0.30146034446454817. Generator loss: 0.44429714641263407. Discriminator loss: 0.11329829404431005. Classification loss: 0.02523479661754062. Regression loss: 0.012742683053910431\n",
      "Epoch 260 loss: 3.5865450277924538. Elbo loss: 0.11115955837791966. Generator loss: 0.44409078744149977. Discriminator loss: 0.11322561050615003. Classification loss: 0.027408383630831996. Regression loss: 0.01531347977147708\n",
      "Epoch 270 loss: 3.609462581574917. Elbo loss: 0.12827266848856403. Generator loss: 0.44466935146239495. Discriminator loss: 0.11274677418893383. Classification loss: 0.020306840092093954. Regression loss: 0.012735051518839394\n",
      "Epoch 280 loss: 3.6709463968873024. Elbo loss: 0.1354497411558705. Generator loss: 0.444146623534541. Discriminator loss: 0.11284101682324563. Classification loss: 0.02264870904506214. Regression loss: 0.013501278838262923\n",
      "Epoch 290 loss: 3.788574457168579. Elbo loss: 0.16663538472306344. Generator loss: 0.4444757036624416. Discriminator loss: 0.11254916027668983. Classification loss: 0.016341397809165138. Regression loss: 0.012648477071648343\n",
      "Epoch 300 loss: 3.5155747085809708. Elbo loss: 0.14074782931035565. Generator loss: 0.4445668651211646. Discriminator loss: 0.11236625765600512. Classification loss: 0.004187649647299681. Regression loss: 0.007030237843507841\n",
      "Epoch 310 loss: 4.436847127974033. Elbo loss: 0.22633703894192173. Generator loss: 0.4442053254573576. Discriminator loss: 0.112444834363076. Classification loss: 0.05170937217940997. Regression loss: 0.01927316653910243\n",
      "Epoch 320 loss: 3.616206906735897. Elbo loss: 0.15494257403958228. Generator loss: 0.4444525934034778. Discriminator loss: 0.11221708765914364. Classification loss: 0.0064682617394869486. Regression loss: 0.007958173364092927\n",
      "Epoch 330 loss: 3.5318873152136803. Elbo loss: 0.13937926052078123. Generator loss: 0.444085753733112. Discriminator loss: 0.11231264856553846. Classification loss: 0.006519008097389052. Regression loss: 0.00842029154732553\n",
      "Epoch 340 loss: 3.3340225219726562. Elbo loss: 0.10455754687709193. Generator loss: 0.4444613168316503. Discriminator loss: 0.11204305819926723. Classification loss: 0.0056373121408415175. Regression loss: 0.009057492810121227\n",
      "Epoch 350 loss: 4.085407063364983. Elbo loss: 0.17534740821969125. Generator loss: 0.4446374754751882. Discriminator loss: 0.11188439904682097. Classification loss: 0.044827654742778486. Regression loss: 0.01742373443219901\n",
      "Epoch 360 loss: 3.522951476275921. Elbo loss: 0.141781207774916. Generator loss: 0.444243170561329. Discriminator loss: 0.11201995442951879. Classification loss: 0.0045768406257153515. Regression loss: 0.007492589739887344\n",
      "Epoch 370 loss: 3.4941156059503555. Elbo loss: 0.13741192558119375. Generator loss: 0.44456684685522513. Discriminator loss: 0.11180003516135677. Classification loss: 0.004050721167474084. Regression loss: 0.007513963325407296\n",
      "Epoch 380 loss: 3.8513422161340714. Elbo loss: 0.16042660585334223. Generator loss: 0.4443134290556754. Discriminator loss: 0.11187051452936665. Classification loss: 0.02917931213479249. Regression loss: 0.01288363559128371\n",
      "Epoch 390 loss: 3.552712008357048. Elbo loss: 0.1519357379405729. Generator loss: 0.44441859087636393. Discriminator loss: 0.1117720123260252. Classification loss: 0.0025069955736398697. Regression loss: 0.005872555739892226\n",
      "Epoch 400 loss: 3.4491207227110863. Elbo loss: 0.1231912953719016. Generator loss: 0.44440723426880374. Discriminator loss: 0.11173600991887431. Classification loss: 0.006502766443337614. Regression loss: 0.010350500581778525\n",
      "Epoch 410 loss: 3.481660045683384. Elbo loss: 0.13563347367509718. Generator loss: 0.444390163306267. Discriminator loss: 0.11170980117974742. Classification loss: 0.004008743628923361. Regression loss: 0.007368930573237219\n",
      "Epoch 420 loss: 3.5466751009225845. Elbo loss: 0.14926102949726966. Generator loss: 0.44447486727468427. Discriminator loss: 0.11162970527525871. Classification loss: 0.0029431008536654015. Regression loss: 0.007002373819329565\n",
      "Epoch 430 loss: 3.6740486696362495. Elbo loss: 0.1562997160419341. Generator loss: 0.44419983317775114. Discriminator loss: 0.11173576573210378. Classification loss: 0.01068434376370973. Regression loss: 0.011841734771047824\n",
      "Epoch 440 loss: 3.5445556342601776. Elbo loss: 0.15043201537862902. Generator loss: 0.444493854238141. Discriminator loss: 0.11155972052966395. Classification loss: 0.002555133300765808. Regression loss: 0.006091134089905198\n",
      "Epoch 450 loss: 3.5490583330392838. Elbo loss: 0.12384195481577227. Generator loss: 0.44445585050890524. Discriminator loss: 0.11155621058517887. Classification loss: 0.01681508128944364. Regression loss: 0.01315124118005887\n",
      "Epoch 460 loss: 3.6054308712482452. Elbo loss: 0.14583633119060146. Generator loss: 0.44426401103696517. Discriminator loss: 0.11162653925918764. Classification loss: 0.011039207431754367. Regression loss: 0.009576130657918722\n",
      "Epoch 470 loss: 3.3656642958521843. Elbo loss: 0.11235430620370372. Generator loss: 0.4443867014300439. Discriminator loss: 0.11154276228720142. Classification loss: 0.004823064034891075. Regression loss: 0.008878719569287158\n",
      "Epoch 480 loss: 3.4047588035464287. Elbo loss: 0.11857409414745146. Generator loss: 0.44443072618976714. Discriminator loss: 0.11149951839639295. Classification loss: 0.005643357917095625. Regression loss: 0.008543617886325886\n",
      "Epoch 490 loss: 3.607885353267193. Elbo loss: 0.11107638021630625. Generator loss: 0.4447303731595316. Discriminator loss: 0.11133311376456291. Classification loss: 0.0329765533510175. Regression loss: 0.013524205814446173\n",
      "Epoch 500 loss: 3.555932618677616. Elbo loss: 0.14785621747855218. Generator loss: 0.44429870959251155. Discriminator loss: 0.11153176691262953. Classification loss: 0.004930132125023632. Regression loss: 0.007498108639171527\n",
      "Epoch 510 loss: 3.40997201949358. Elbo loss: 0.11154244383496623. Generator loss: 0.4442879913314696. Discriminator loss: 0.11152206890044673. Classification loss: 0.007438862488338453. Regression loss: 0.01401694035795968\n",
      "Epoch 520 loss: 3.423422187566757. Elbo loss: 0.12810227491201892. Generator loss: 0.444525606209232. Discriminator loss: 0.11139120690284236. Classification loss: 0.0026581226486615058. Regression loss: 0.006721268459636298\n",
      "Epoch 530 loss: 3.5561457574367523. Elbo loss: 0.12973656553414561. Generator loss: 0.4444256809449965. Discriminator loss: 0.11142809088191678. Classification loss: 0.016289688041245927. Regression loss: 0.00948553931012582\n",
      "Epoch 540 loss: 3.5161306262016296. Elbo loss: 0.1466234758976967. Generator loss: 0.4445367022868126. Discriminator loss: 0.11136380078331116. Classification loss: 0.0016738605364451124. Regression loss: 0.006299270341368092\n",
      "Epoch 550 loss: 3.453661821782589. Elbo loss: 0.13209783189719723. Generator loss: 0.4445290325149413. Discriminator loss: 0.11135563206288122. Classification loss: 0.0028003900210493275. Regression loss: 0.008005616047622396\n",
      "Epoch 560 loss: 3.394633837044239. Elbo loss: 0.1206801718761844. Generator loss: 0.4444869141424856. Discriminator loss: 0.1113652394663903. Classification loss: 0.0036155933562587316. Regression loss: 0.00766633766385982\n",
      "Epoch 570 loss: 3.459507055580616. Elbo loss: 0.13752629631950009. Generator loss: 0.44442714798835015. Discriminator loss: 0.1113859458315757. Classification loss: 0.0015108660929047714. Regression loss: 0.005499323259315251\n",
      "Epoch 580 loss: 3.7863084599375725. Elbo loss: 0.14465845063809427. Generator loss: 0.4444409974159733. Discriminator loss: 0.11136955191050807. Classification loss: 0.031727621810733256. Regression loss: 0.013947800862708038\n",
      "Epoch 590 loss: 3.3892696276307106. Elbo loss: 0.12266454076574694. Generator loss: 0.4443346290819107. Discriminator loss: 0.11141794775762866. Classification loss: 0.0024519056225413335. Regression loss: 0.006413784462981106\n",
      "Epoch 600 loss: 3.4251242727041245. Elbo loss: 0.13073326166599028. Generator loss: 0.4443128945366029. Discriminator loss: 0.11142024830464393. Classification loss: 0.0017615152586762223. Regression loss: 0.005787493771358934\n",
      "Epoch 610 loss: 3.5465134903788567. Elbo loss: 0.1335973984772159. Generator loss: 0.4447206239546499. Discriminator loss: 0.11120983165118002. Classification loss: 0.011886905939102308. Regression loss: 0.01028438137314703\n",
      "Epoch 620 loss: 3.4798144474625587. Elbo loss: 0.13133567643742408. Generator loss: 0.44428739720775234. Discriminator loss: 0.11141966307355512. Classification loss: 0.006573466748851652. Regression loss: 0.008159030066904492\n",
      "Epoch 630 loss: 3.4099904373288155. Elbo loss: 0.12627430596659261. Generator loss: 0.44447955873704725. Discriminator loss: 0.11131922131584536. Classification loss: 0.0024077794031097345. Regression loss: 0.006628588409042887\n",
      "Epoch 640 loss: 3.434975028038025. Elbo loss: 0.11803682292661359. Generator loss: 0.44454475948887484. Discriminator loss: 0.11128081886037704. Classification loss: 0.009050117209910476. Regression loss: 0.009875632310849465\n",
      "Epoch 650 loss: 3.479715511202812. Elbo loss: 0.11533720839408136. Generator loss: 0.444424694584262. Discriminator loss: 0.1113355121304912. Classification loss: 0.01641690509695177. Regression loss: 0.01009980161700608\n",
      "Epoch 660 loss: 3.402512528002262. Elbo loss: 0.12170672200379833. Generator loss: 0.4444696153363874. Discriminator loss: 0.11130903348807365. Classification loss: 0.0031504083838832055. Regression loss: 0.008812972549593167\n",
      "Epoch 670 loss: 3.7113544791936874. Elbo loss: 0.11750476134400215. Generator loss: 0.4443122360014146. Discriminator loss: 0.11139035945938479. Classification loss: 0.039997763457875174. Regression loss: 0.015878256067686204\n",
      "Epoch 680 loss: 3.4274573996663094. Elbo loss: 0.12948680332591456. Generator loss: 0.4443680519057858. Discriminator loss: 0.11135241918025478. Classification loss: 0.002409142928832631. Regression loss: 0.006585599709069356\n",
      "Epoch 690 loss: 3.4162637293338776. Elbo loss: 0.12321318661974322. Generator loss: 0.444546849496903. Discriminator loss: 0.11126349121332169. Classification loss: 0.004517702167504467. Regression loss: 0.00783454245069034\n",
      "Epoch 700 loss: 3.4765160530805588. Elbo loss: 0.12592962600531116. Generator loss: 0.44447813303239886. Discriminator loss: 0.11129062334375997. Classification loss: 0.008423933042297816. Regression loss: 0.010376849583060961\n",
      "Epoch 710 loss: 3.617504358291626. Elbo loss: 0.13793197009832628. Generator loss: 0.4443375083707994. Discriminator loss: 0.11135649993535011. Classification loss: 0.01769339708651739. Regression loss: 0.010477671957379508\n",
      "Epoch 720 loss: 3.360139638185501. Elbo loss: 0.11711352679037279. Generator loss: 0.4444064026878726. Discriminator loss: 0.11131977698495311. Classification loss: 0.002144929887670573. Regression loss: 0.007256967696218541\n",
      "Epoch 730 loss: 3.3613332584500313. Elbo loss: 0.11493073548040082. Generator loss: 0.4444045424461365. Discriminator loss: 0.1113171998050905. Classification loss: 0.0037892350559442065. Regression loss: 0.007310292616884615\n",
      "Epoch 740 loss: 3.370653346180916. Elbo loss: 0.11412454805066509. Generator loss: 0.44451460722954045. Discriminator loss: 0.11126039417520646. Classification loss: 0.003941659625730432. Regression loss: 0.009576293515912708\n",
      "Epoch 750 loss: 3.7656154111027718. Elbo loss: 0.11743528227652272. Generator loss: 0.44434217964449235. Discriminator loss: 0.11134362124627636. Classification loss: 0.04704151186888339. Regression loss: 0.01568256518916197\n",
      "Epoch 760 loss: 3.387548603117466. Elbo loss: 0.12102485904770513. Generator loss: 0.4444396765001358. Discriminator loss: 0.11129201323755326. Classification loss: 0.0030767502844795554. Regression loss: 0.006956393480058685\n",
      "Epoch 770 loss: 3.3381432369351387. Elbo loss: 0.11075342734975199. Generator loss: 0.4443911438988101. Discriminator loss: 0.11131460027348611. Classification loss: 0.003236386309576882. Regression loss: 0.00812287667987778\n",
      "Epoch 780 loss: 3.491653673350811. Elbo loss: 0.1187430557704741. Generator loss: 0.44451937848521816. Discriminator loss: 0.11124826414931205. Classification loss: 0.014205092830619128. Regression loss: 0.012056263756089574\n",
      "Epoch 790 loss: 3.4226401448249817. Elbo loss: 0.1238465849911013. Generator loss: 0.444481841018123. Discriminator loss: 0.11126437639036486. Classification loss: 0.0045114237176265655. Regression loss: 0.008401272081462066\n",
      "Epoch 800 loss: 3.369155414402485. Elbo loss: 0.12052016609138058. Generator loss: 0.4444135619748023. Discriminator loss: 0.1112967594977348. Classification loss: 0.0016063117133941682. Regression loss: 0.006267896350172739\n",
      "Epoch 810 loss: 3.3348152488470078. Elbo loss: 0.11312240770747585. Generator loss: 0.4444593854488865. Discriminator loss: 0.11127251771188551. Classification loss: 0.0020247846623533405. Regression loss: 0.006898005349343763\n",
      "Epoch 820 loss: 3.458615943789482. Elbo loss: 0.11636611795233141. Generator loss: 0.4443929041585615. Discriminator loss: 0.11130463620347361. Classification loss: 0.014353258854938641. Regression loss: 0.008293692176530679\n",
      "Epoch 830 loss: 3.3708552792668343. Elbo loss: 0.11471784907963968. Generator loss: 0.44453824239392437. Discriminator loss: 0.11123232038751725. Classification loss: 0.0053128618784549254. Regression loss: 0.007083637254489135\n",
      "Epoch 840 loss: 3.3867161944508553. Elbo loss: 0.11918205239119069. Generator loss: 0.4444150770864179. Discriminator loss: 0.11129080936793358. Classification loss: 0.003416159544881779. Regression loss: 0.008184017361922851\n",
      "Epoch 850 loss: 3.369751863181591. Elbo loss: 0.11891874046094957. Generator loss: 0.4444584125472653. Discriminator loss: 0.11126655965082106. Classification loss: 0.0023480668650365496. Regression loss: 0.006927983687708019\n",
      "Epoch 860 loss: 3.526994898915291. Elbo loss: 0.12536565118259. Generator loss: 0.44453583801946334. Discriminator loss: 0.11122977373100096. Classification loss: 0.01649466554672765. Regression loss: 0.008529151974324767\n",
      "Epoch 870 loss: 3.4077081382274628. Elbo loss: 0.1206470723113706. Generator loss: 0.4444835368664034. Discriminator loss: 0.1112515601900316. Classification loss: 0.005256055078315248. Regression loss: 0.007870489377490875\n",
      "Epoch 880 loss: 3.3739457726478577. Elbo loss: 0.11711649933168965. Generator loss: 0.4442810272016833. Discriminator loss: 0.11135247517977992. Classification loss: 0.003150473090851935. Regression loss: 0.00834164193422053\n",
      "Epoch 890 loss: 3.365190029144287. Elbo loss: 0.11812321601375457. Generator loss: 0.4443932867819263. Discriminator loss: 0.11129435440224986. Classification loss: 0.002509973844186583. Regression loss: 0.006679156782939249\n",
      "Epoch 900 loss: 3.406439356505871. Elbo loss: 0.12086272335821582. Generator loss: 0.44442208447764. Discriminator loss: 0.1112799721379434. Classification loss: 0.004286993594676812. Regression loss: 0.008814493616227992\n",
      "Epoch 910 loss: 3.379695847630501. Elbo loss: 0.12227909242914568. Generator loss: 0.4445233383486348. Discriminator loss: 0.11122747198227913. Classification loss: 0.002039657843265412. Regression loss: 0.005804839456217333\n",
      "Epoch 920 loss: 3.635609954595566. Elbo loss: 0.1177590506211404. Generator loss: 0.4444829667768171. Discriminator loss: 0.11124753158900046. Classification loss: 0.032706832904002496. Regression loss: 0.012508597208990625\n",
      "Epoch 930 loss: 3.387214243412018. Elbo loss: 0.12180995148035788. Generator loss: 0.444307005213153. Discriminator loss: 0.11133436498142057. Classification loss: 0.002466248250481928. Regression loss: 0.0070176153822103515\n",
      "Epoch 940 loss: 3.435462288558483. Elbo loss: 0.12035904271948722. Generator loss: 0.4444327094862538. Discriminator loss: 0.1112708035976656. Classification loss: 0.009106833886753226. Regression loss: 0.00765431237467114\n",
      "Epoch 950 loss: 3.3824803829193115. Elbo loss: 0.11968196664125688. Generator loss: 0.4443742036819458. Discriminator loss: 0.1113019484666086. Classification loss: 0.002999336198451103. Regression loss: 0.00753157120949078\n",
      "Epoch 960 loss: 3.3980800211429596. Elbo loss: 0.12269361413294269. Generator loss: 0.4443349242210388. Discriminator loss: 0.11131865411035476. Classification loss: 0.003036286749486481. Regression loss: 0.007274958938303884\n",
      "Epoch 970 loss: 3.3738961294293404. Elbo loss: 0.11697274662794606. Generator loss: 0.4444995887817875. Discriminator loss: 0.11123540228412997. Classification loss: 0.0037412408975444943. Regression loss: 0.00764364727600027\n",
      "Epoch 980 loss: 3.3872457817196846. Elbo loss: 0.11871494665261238. Generator loss: 0.44445255879432927. Discriminator loss: 0.11125766413827096. Classification loss: 0.0036617232488483312. Regression loss: 0.008416077806501679\n",
      "Epoch 990 loss: 3.3680964782834053. Elbo loss: 0.11407032608985901. Generator loss: 0.4442572747507403. Discriminator loss: 0.11135502520107454. Classification loss: 0.004837773222759772. Regression loss: 0.007939462198098022\n",
      "Epoch 1000 loss: 3.4488556683063507. Elbo loss: 0.12005926548473296. Generator loss: 0.4442640312256352. Discriminator loss: 0.11135060916985234. Classification loss: 0.009293482200929805. Regression loss: 0.010101020030091487\n",
      "Epoch 1010 loss: 3.391026757657528. Elbo loss: 0.11983050069501323. Generator loss: 0.4444948655943717. Discriminator loss: 0.11123538113409473. Classification loss: 0.004102829290786758. Regression loss: 0.007351517624732467\n",
      "Epoch 1020 loss: 3.4826527163386345. Elbo loss: 0.11760396510362625. Generator loss: 0.4440886032196783. Discriminator loss: 0.11143825587726408. Classification loss: 0.015586823994395203. Regression loss: 0.009640051558474829\n",
      "Epoch 1030 loss: 3.3768578842282295. Elbo loss: 0.11975226791635636. Generator loss: 0.44444479288593414. Discriminator loss: 0.1112622794124388. Classification loss: 0.002225240243264594. Regression loss: 0.007562878055332769\n",
      "Epoch 1040 loss: 3.3691796958446503. Elbo loss: 0.11932760116554075. Generator loss: 0.44441902060662547. Discriminator loss: 0.11127339399630023. Classification loss: 0.001976900429121264. Regression loss: 0.006964903410276278\n",
      "Epoch 1050 loss: 3.4243355467915535. Elbo loss: 0.1216627340162954. Generator loss: 0.44449089034911127. Discriminator loss: 0.11123553254911976. Classification loss: 0.005586733295199191. Regression loss: 0.009388415593533747\n",
      "Epoch 1060 loss: 3.394883118569851. Elbo loss: 0.12315620770377497. Generator loss: 0.44443284119329146. Discriminator loss: 0.11126381831784402. Classification loss: 0.0024428419264656822. Regression loss: 0.007093984316358523\n",
      "Epoch 1070 loss: 3.4193259701132774. Elbo loss: 0.11834750733067913. Generator loss: 0.4444261798935552. Discriminator loss: 0.11126652071552892. Classification loss: 0.007277298942133184. Regression loss: 0.00939016200948484\n",
      "Epoch 1080 loss: 3.370028704404831. Elbo loss: 0.1198627814169853. Generator loss: 0.4442157764588633. Discriminator loss: 0.11137122612807059. Classification loss: 0.0018482340895007515. Regression loss: 0.006771525603741588\n",
      "Epoch 1090 loss: 3.3873329386115074. Elbo loss: 0.11907206331529925. Generator loss: 0.44435748361772104. Discriminator loss: 0.11129981111134252. Classification loss: 0.004070434962481921. Regression loss: 0.007498455327755261\n",
      "Epoch 1100 loss: 3.380006492137909. Elbo loss: 0.11741130294338349. Generator loss: 0.4444627906045606. Discriminator loss: 0.11124773635979622. Classification loss: 0.004100073236435081. Regression loss: 0.007799300757582269\n",
      "Epoch 1110 loss: 3.3580389618873596. Elbo loss: 0.12030569968685027. Generator loss: 0.44436410165602164. Discriminator loss: 0.11129668571295277. Classification loss: 0.0012060057039977232. Regression loss: 0.005091788157099678\n",
      "Epoch 1120 loss: 3.364450193941593. Elbo loss: 0.1157703445323052. Generator loss: 0.4443792643085603. Discriminator loss: 0.11128820743291609. Classification loss: 0.003554612596476105. Regression loss: 0.007425509296045939\n",
      "Epoch 1130 loss: 3.3764844238758087. Elbo loss: 0.11505390631575738. Generator loss: 0.4443329236199779. Discriminator loss: 0.11131055340651543. Classification loss: 0.005975591488447295. Regression loss: 0.0068474302796887295\n",
      "Epoch 1140 loss: 3.3565741926431656. Elbo loss: 0.11267802864313126. Generator loss: 0.44448093156660756. Discriminator loss: 0.11124073906290916. Classification loss: 0.003281169469353758. Regression loss: 0.00948814170542098\n",
      "Epoch 1150 loss: 3.3967048302292824. Elbo loss: 0.1147431154404917. Generator loss: 0.4446263601703028. Discriminator loss: 0.11116421727403518. Classification loss: 0.006764837406432977. Regression loss: 0.009664264368644405\n",
      "Epoch 1160 loss: 3.36163442581892. Elbo loss: 0.11682135683874931. Generator loss: 0.4446106110849688. Discriminator loss: 0.11117250688614384. Classification loss: 0.002693201840062806. Regression loss: 0.007099071637522655\n",
      "Epoch 1170 loss: 3.4569648429751396. Elbo loss: 0.11940509922081424. Generator loss: 0.44442080682323826. Discriminator loss: 0.11126733907768803. Classification loss: 0.010307681188879219. Regression loss: 0.010770713446676851\n",
      "Epoch 1180 loss: 3.395991273224354. Elbo loss: 0.11830287667051438. Generator loss: 0.4441763037635434. Discriminator loss: 0.11138839683225078. Classification loss: 0.0058194741098803555. Regression loss: 0.007327980160029725\n",
      "Epoch 1190 loss: 3.364712670445442. Elbo loss: 0.1171070457466187. Generator loss: 0.44439857332937177. Discriminator loss: 0.1112772074918593. Classification loss: 0.002806329455310216. Regression loss: 0.007206882205566451\n",
      "Epoch 1200 loss: 3.380047284066677. Elbo loss: 0.11853427175552614. Generator loss: 0.4442576410308961. Discriminator loss: 0.11134889385392589. Classification loss: 0.0037953816917574695. Regression loss: 0.00712097869461192\n",
      "Epoch 1210 loss: 3.411057285964489. Elbo loss: 0.11875790237419066. Generator loss: 0.4445302313373935. Discriminator loss: 0.11121101773554279. Classification loss: 0.006395419155465319. Regression loss: 0.008758496985988571\n",
      "Epoch 1220 loss: 3.37396577000618. Elbo loss: 0.1189391415926718. Generator loss: 0.44456621139280256. Discriminator loss: 0.11119217497687187. Classification loss: 0.002870829395232572. Regression loss: 0.006947377127700395\n",
      "Epoch 1230 loss: 3.3367425203323364. Elbo loss: 0.11551335021372765. Generator loss: 0.4445124989555728. Discriminator loss: 0.11122005697219603. Classification loss: 0.0015236404517875804. Regression loss: 0.005606360052854965\n",
      "Epoch 1240 loss: 3.3624895587563515. Elbo loss: 0.11448836230462597. Generator loss: 0.444425682867727. Discriminator loss: 0.11126234694834679. Classification loss: 0.004027950232886078. Regression loss: 0.007682701646943667\n",
      "Epoch 1250 loss: 3.398229204118252. Elbo loss: 0.1170479376469889. Generator loss: 0.44447608147898027. Discriminator loss: 0.11123721902408908. Classification loss: 0.007271589503591625. Regression loss: 0.006912523152066335\n",
      "Epoch 1260 loss: 3.343437746167183. Elbo loss: 0.11516080988991645. Generator loss: 0.4442970060533093. Discriminator loss: 0.1113265368246263. Classification loss: 0.0019454110172979022. Regression loss: 0.0065614837644276235\n",
      "Epoch 1270 loss: 3.3800939545035362. Elbo loss: 0.11724140422959481. Generator loss: 0.4442162254164296. Discriminator loss: 0.11136711484962894. Classification loss: 0.00422714184500247. Regression loss: 0.007808975006018074\n",
      "Epoch 1280 loss: 3.3922284841537476. Elbo loss: 0.11695366328762423. Generator loss: 0.44451221631419274. Discriminator loss: 0.11121853032419758. Classification loss: 0.0050673188304626235. Regression loss: 0.009078161272731039\n",
      "Epoch 1290 loss: 3.3848488852381706. Elbo loss: 0.11623785981247502. Generator loss: 0.44471551622113875. Discriminator loss: 0.11111655062244784. Classification loss: 0.005003963163143189. Regression loss: 0.008558661119276357\n",
      "Epoch 1300 loss: 3.4115636870265007. Elbo loss: 0.1182495903584265. Generator loss: 0.4443511751390273. Discriminator loss: 0.11129863824575179. Classification loss: 0.006257089337469229. Regression loss: 0.009558144108591331\n",
      "Epoch 1310 loss: 3.3570505902171135. Elbo loss: 0.11645068324381305. Generator loss: 0.44438999410598506. Discriminator loss: 0.11127924126002096. Classification loss: 0.002357460035249636. Regression loss: 0.007130653791752975\n",
      "Epoch 1320 loss: 3.3812924176454544. Elbo loss: 0.11897977393481039. Generator loss: 0.4444978121788271. Discriminator loss: 0.11122575642601136. Classification loss: 0.0032563621665054423. Regression loss: 0.0076736387507539364\n",
      "Epoch 1330 loss: 3.522847905755043. Elbo loss: 0.11640696011243327. Generator loss: 0.4444636519878141. Discriminator loss: 0.11124270697755198. Classification loss: 0.01909843528330619. Regression loss: 0.013055945382333331\n",
      "Epoch 1340 loss: 3.532360315322876. Elbo loss: 0.11749310479048759. Generator loss: 0.44439845027462127. Discriminator loss: 0.11127796360561924. Classification loss: 0.020008563748448185. Regression loss: 0.012375438650656913\n",
      "Epoch 1350 loss: 3.3672927543520927. Elbo loss: 0.11766728734777819. Generator loss: 0.44463334064329824. Discriminator loss: 0.11115715893045548. Classification loss: 0.0024651226618340343. Regression loss: 0.007601585881333918\n",
      "Epoch 1360 loss: 3.395482122898102. Elbo loss: 0.1176236312716238. Generator loss: 0.4444057124276315. Discriminator loss: 0.11127101750143113. Classification loss: 0.00596899497994004. Regression loss: 0.00770594549577518\n",
      "Epoch 1370 loss: 3.3769804164767265. Elbo loss: 0.11773345619440079. Generator loss: 0.44463323008629585. Discriminator loss: 0.11115659124428226. Classification loss: 0.0038202863157024577. Regression loss: 0.007341463094381916\n",
      "Epoch 1380 loss: 3.4376265630126. Elbo loss: 0.1202788939399104. Generator loss: 0.44437566303437753. Discriminator loss: 0.11128636641848472. Classification loss: 0.009139050792709309. Regression loss: 0.008095911112069453\n",
      "Epoch 1390 loss: 3.4270272627472878. Elbo loss: 0.12063129774985774. Generator loss: 0.44419671258618754. Discriminator loss: 0.11137547632378916. Classification loss: 0.007598420617033562. Regression loss: 0.00803999897142339\n",
      "Epoch 1400 loss: 3.3734495639801025. Elbo loss: 0.11967802456309719. Generator loss: 0.444371162883697. Discriminator loss: 0.11129138738878312. Classification loss: 0.0026525220429551065. Regression loss: 0.006425171200516483\n",
      "Epoch 1410 loss: 3.353269502520561. Elbo loss: 0.1182038065406584. Generator loss: 0.4445412899217298. Discriminator loss: 0.11120294539197799. Classification loss: 0.001641092756941479. Regression loss: 0.0057222812115909506\n",
      "Epoch 1420 loss: 3.427947275340557. Elbo loss: 0.11911891953599069. Generator loss: 0.44442160475638604. Discriminator loss: 0.11126251182248516. Classification loss: 0.008688940614745426. Regression loss: 0.008160568501015434\n",
      "Epoch 1430 loss: 3.5518076196312904. Elbo loss: 0.11872541135357272. Generator loss: 0.4445190275869062. Discriminator loss: 0.1112140289718105. Classification loss: 0.021699440164206128. Regression loss: 0.01222794023042004\n",
      "Epoch 1440 loss: 3.367685616016388. Elbo loss: 0.11871452509395537. Generator loss: 0.4444225449715891. Discriminator loss: 0.11126218447762151. Classification loss: 0.002630707315420882. Regression loss: 0.006388942653786451\n",
      "Epoch 1450 loss: 3.426956295967102. Elbo loss: 0.11931862561933455. Generator loss: 0.44448199579792635. Discriminator loss: 0.11123369489946673. Classification loss: 0.008149831999147747. Regression loss: 0.00855095528259558\n",
      "Epoch 1460 loss: 3.3865032494068146. Elbo loss: 0.11749827933888282. Generator loss: 0.44428150500020674. Discriminator loss: 0.11133266264392484. Classification loss: 0.004788504982218823. Regression loss: 0.0079046493272237\n",
      "Epoch 1470 loss: 3.3765889927744865. Elbo loss: 0.11953406204139033. Generator loss: 0.4443532276538111. Discriminator loss: 0.11129669172148551. Classification loss: 0.0025022703743044048. Regression loss: 0.007353251053568636\n",
      "Epoch 1480 loss: 3.3472025245428085. Elbo loss: 0.11774014681577682. Generator loss: 0.44440214383986687. Discriminator loss: 0.11127169766733723. Classification loss: 0.0012473889646278094. Regression loss: 0.005659179152703033\n",
      "Epoch 1490 loss: 3.383924312889576. Elbo loss: 0.11759929863675948. Generator loss: 0.44427940249443054. Discriminator loss: 0.11133340962471501. Classification loss: 0.004743056705563269. Regression loss: 0.007405484439767597\n",
      "Epoch 1500 loss: 3.413053445518017. Elbo loss: 0.11816890609841194. Generator loss: 0.4442889594262646. Discriminator loss: 0.11132939231972541. Classification loss: 0.00670983312386764. Regression loss: 0.009260313303750609\n",
      "Epoch 1510 loss: 3.3742970302700996. Elbo loss: 0.11809657946709663. Generator loss: 0.44427668375353657. Discriminator loss: 0.11133482331229795. Classification loss: 0.0033570407336341187. Regression loss: 0.007157910234039469\n",
      "Epoch 1520 loss: 3.3846954628825188. Elbo loss: 0.11789196681591772. Generator loss: 0.44432803411637584. Discriminator loss: 0.11130901281872103. Classification loss: 0.0035101768617976396. Regression loss: 0.009012797841757176\n",
      "Epoch 1530 loss: 3.3708758801221848. Elbo loss: 0.11761396786858959. Generator loss: 0.4443996183333858. Discriminator loss: 0.11127280035326557. Classification loss: 0.0035191464485251345. Regression loss: 0.006794884565263806\n",
      "Epoch 1540 loss: 3.3876961022615433. Elbo loss: 0.11884986152572016. Generator loss: 0.444188222769768. Discriminator loss: 0.11138328453225474. Classification loss: 0.0037929732368924777. Regression loss: 0.008183947242345781\n",
      "Epoch 1550 loss: 3.3641718477010727. Elbo loss: 0.11758597844070004. Generator loss: 0.44417168055811235. Discriminator loss: 0.11138701679245118. Classification loss: 0.002600855258224915. Regression loss: 0.006929489078878936\n",
      "Epoch 1560 loss: 3.4606038853526115. Elbo loss: 0.11726845993149665. Generator loss: 0.44454759263223215. Discriminator loss: 0.11119818999882668. Classification loss: 0.012362033320025676. Regression loss: 0.0106348056870047\n",
      "Epoch 1570 loss: 3.4658879190683365. Elbo loss: 0.11856668971238597. Generator loss: 0.44412510914187275. Discriminator loss: 0.11141125617488738. Classification loss: 0.013347143854668004. Regression loss: 0.008876336266938222\n",
      "Epoch 1580 loss: 3.351358413696289. Elbo loss: 0.11858624436201588. Generator loss: 0.4444322403400175. Discriminator loss: 0.111257286081391. Classification loss: 0.0013856873096477602. Regression loss: 0.005361509502343383\n",
      "Epoch 1590 loss: 3.3662402778863907. Elbo loss: 0.11831526217922088. Generator loss: 0.44436211355270877. Discriminator loss: 0.11129188753904835. Classification loss: 0.002382347714662882. Regression loss: 0.006883973616265481\n",
      "Epoch 1600 loss: 3.389846421778202. Elbo loss: 0.11821330074341066. Generator loss: 0.4443720723352125. Discriminator loss: 0.11128550335284203. Classification loss: 0.0036968650904694394. Regression loss: 0.009352631656633269\n",
      "Epoch 1610 loss: 3.3797348737716675. Elbo loss: 0.11678160534751031. Generator loss: 0.4441959415712664. Discriminator loss: 0.11137713323677739. Classification loss: 0.004225200486431394. Regression loss: 0.008207185803199067\n",
      "Epoch 1620 loss: 3.3645641580224037. Elbo loss: 0.11701382600492047. Generator loss: 0.4444123679591763. Discriminator loss: 0.11126923368823144. Classification loss: 0.0027047331184862303. Regression loss: 0.007420090825161746\n",
      "Epoch 1630 loss: 3.348404049873352. Elbo loss: 0.1168375440662907. Generator loss: 0.4444089089670489. Discriminator loss: 0.11126790315874162. Classification loss: 0.00178986819951059. Regression loss: 0.00600333410200678\n",
      "Epoch 1640 loss: 3.3558723628520966. Elbo loss: 0.11683931466071837. Generator loss: 0.44444533221183286. Discriminator loss: 0.11125069544199974. Classification loss: 0.0022139727830841776. Regression loss: 0.006736497976817191\n",
      "Epoch 1650 loss: 3.38594900816679. Elbo loss: 0.11673314869403839. Generator loss: 0.44447590170368073. Discriminator loss: 0.11123541285914759. Classification loss: 0.005343615648011872. Regression loss: 0.007777228610657696\n",
      "Epoch 1660 loss: 3.41581704467535. Elbo loss: 0.11701225417275582. Generator loss: 0.4445051195160035. Discriminator loss: 0.1112201165768408. Classification loss: 0.007607453210268079. Regression loss: 0.009633039327097996\n",
      "Epoch 1670 loss: 3.408393308520317. Elbo loss: 0.11844539570231591. Generator loss: 0.4445747002478569. Discriminator loss: 0.11118493878072308. Classification loss: 0.006120094756038123. Regression loss: 0.008991017457916431\n",
      "Epoch 1680 loss: 3.3831237107515335. Elbo loss: 0.11758233318405767. Generator loss: 0.4444348869785186. Discriminator loss: 0.11125585436820984. Classification loss: 0.004372516150103747. Regression loss: 0.007807642044011323\n",
      "Epoch 1690 loss: 3.360538028180599. Elbo loss: 0.1165134156903913. Generator loss: 0.444594064066487. Discriminator loss: 0.11117526263959947. Classification loss: 0.00264997897348592. Regression loss: 0.007279593025281605\n",
      "Epoch 1700 loss: 3.548594430088997. Elbo loss: 0.11892823586540838. Generator loss: 0.4444642836047757. Discriminator loss: 0.11124070132932355. Classification loss: 0.02258713377124232. Regression loss: 0.010180484088073906\n",
      "Epoch 1710 loss: 3.359908439218998. Elbo loss: 0.11861888319253922. Generator loss: 0.4444030571368433. Discriminator loss: 0.11127082426701823. Classification loss: 0.0018312312050482197. Regression loss: 0.006230522673098265\n",
      "Epoch 1720 loss: 3.347413592040539. Elbo loss: 0.11782959992847135. Generator loss: 0.44425182092574333. Discriminator loss: 0.11134714368850954. Classification loss: 0.001386895872729515. Regression loss: 0.005407955762723671\n",
      "Epoch 1730 loss: 3.367405079305172. Elbo loss: 0.11935597730259742. Generator loss: 0.4444146800425745. Discriminator loss: 0.111266236632101. Classification loss: 0.0021600469316904163. Regression loss: 0.0063692649728768775\n",
      "Epoch 1740 loss: 3.41214781999588. Elbo loss: 0.11792056094254216. Generator loss: 0.44438796850942797. Discriminator loss: 0.11127845750701043. Classification loss: 0.007538570871298772. Regression loss: 0.008164023236003555\n",
      "Epoch 1750 loss: 3.3853202536702156. Elbo loss: 0.11766626036936237. Generator loss: 0.4446205198764801. Discriminator loss: 0.11116216812402971. Classification loss: 0.004576697257899439. Regression loss: 0.007828051079770611\n",
      "Epoch 1760 loss: 3.4099326878786087. Elbo loss: 0.11892788977392259. Generator loss: 0.44449215262166913. Discriminator loss: 0.1112263320434478. Classification loss: 0.005828745430100694. Regression loss: 0.009201229893557367\n",
      "Epoch 1770 loss: 3.603270500898361. Elbo loss: 0.1164517856894001. Generator loss: 0.44430434511553857. Discriminator loss: 0.111320860683918. Classification loss: 0.030338108748002277. Regression loss: 0.011410421282503634\n",
      "Epoch 1780 loss: 3.355597123503685. Elbo loss: 0.11654862184678355. Generator loss: 0.4444677224082331. Discriminator loss: 0.11123895837414649. Classification loss: 0.002359106463582946. Regression loss: 0.006771171824150388\n",
      "Epoch 1790 loss: 3.3909252285957336. Elbo loss: 0.1165329052075263. Generator loss: 0.44457417053561055. Discriminator loss: 0.11118523391985125. Classification loss: 0.005419857987186744. Regression loss: 0.008764385643805708\n",
      "Epoch 1800 loss: 3.587769106030464. Elbo loss: 0.11852893425572303. Generator loss: 0.4443564126568456. Discriminator loss: 0.11129384151389522. Classification loss: 0.027048224429299515. Regression loss: 0.011248552354968964\n",
      "Epoch 1810 loss: 3.354709066450596. Elbo loss: 0.11682451804799418. Generator loss: 0.4444146012106249. Discriminator loss: 0.11126541586652879. Classification loss: 0.002178246577621453. Regression loss: 0.00659417882261257\n",
      "Epoch 1820 loss: 3.411789320409298. Elbo loss: 0.12036331190216926. Generator loss: 0.4445565987017847. Discriminator loss: 0.11119464712758217. Classification loss: 0.00472906521885776. Regression loss: 0.00966969606721954\n",
      "Epoch 1830 loss: 3.6925130411982536. Elbo loss: 0.11660667172362728. Generator loss: 0.4445577648378188. Discriminator loss: 0.11119375858576067. Classification loss: 0.041739935034955475. Regression loss: 0.011006762739270926\n",
      "Epoch 1840 loss: 3.3545463904738426. Elbo loss: 0.11677331237062331. Generator loss: 0.4443817061762656. Discriminator loss: 0.11128195807818443. Classification loss: 0.0018023579611889868. Regression loss: 0.007153002686062527\n",
      "Epoch 1850 loss: 3.3823955953121185. Elbo loss: 0.11890835699535185. Generator loss: 0.4444182015234424. Discriminator loss: 0.11126384884119034. Classification loss: 0.003700104431059557. Regression loss: 0.0073118586433044966\n",
      "Epoch 1860 loss: 3.366868406534195. Elbo loss: 0.11828889096936872. Generator loss: 0.4445688666835908. Discriminator loss: 0.1111877974002592. Classification loss: 0.002849860259307353. Regression loss: 0.006356380979395321\n",
      "Epoch 1870 loss: 3.38362780213356. Elbo loss: 0.11700978274306943. Generator loss: 0.444458665386323. Discriminator loss: 0.11124284445278106. Classification loss: 0.004585309400769596. Regression loss: 0.008168402797309682\n",
      "Epoch 1880 loss: 3.35136641561985. Elbo loss: 0.1169500362969214. Generator loss: 0.4443518692447293. Discriminator loss: 0.11129697604525474. Classification loss: 0.0021464703719586043. Regression loss: 0.005913032673209185\n",
      "Epoch 1890 loss: 3.3684671595692635. Elbo loss: 0.11686942558134755. Generator loss: 0.4444279766851856. Discriminator loss: 0.11125921530108299. Classification loss: 0.0030629557997247413. Regression loss: 0.00775736391228894\n",
      "Epoch 1900 loss: 3.3487057834863663. Elbo loss: 0.11693590975576831. Generator loss: 0.44431605069868024. Discriminator loss: 0.11131518430286838. Classification loss: 0.0015014183485501206. Regression loss: 0.006370044387539008\n",
      "Epoch 1910 loss: 3.3704050704836845. Elbo loss: 0.11681222891615282. Generator loss: 0.4444259741613942. Discriminator loss: 0.11125976472131667. Classification loss: 0.004234420300851907. Regression loss: 0.006489968331034986\n",
      "Epoch 1920 loss: 3.3788999170064926. Elbo loss: 0.11677475129404376. Generator loss: 0.44425539143623843. Discriminator loss: 0.11134493855699416. Classification loss: 0.004956973739852754. Regression loss: 0.007022130608205653\n",
      "Epoch 1930 loss: 3.395886108279228. Elbo loss: 0.11881423453169485. Generator loss: 0.44440890319885745. Discriminator loss: 0.11126774140904026. Classification loss: 0.005230349367681605. Regression loss: 0.007645557461787135\n",
      "Epoch 1940 loss: 3.5006381273269653. Elbo loss: 0.1169454897603681. Generator loss: 0.44437441037547204. Discriminator loss: 0.11128597273941963. Classification loss: 0.017739189247931202. Regression loss: 0.010469514918851576\n",
      "Epoch 1950 loss: 3.3842755034565926. Elbo loss: 0.11939844993814346. Generator loss: 0.4445099628740741. Discriminator loss: 0.11121737957000732. Classification loss: 0.0036431529862907593. Regression loss: 0.007241832046401536\n",
      "Epoch 1960 loss: 3.382988728582859. Elbo loss: 0.11677404541161752. Generator loss: 0.44453663114578496. Discriminator loss: 0.11120467512838302. Classification loss: 0.004505420452915132. Regression loss: 0.00840225721654978\n",
      "Epoch 1970 loss: 3.345708981156349. Elbo loss: 0.11698023710520036. Generator loss: 0.4445059376378213. Discriminator loss: 0.11121974212508048. Classification loss: 0.00132123316777119. Regression loss: 0.006045250117876416\n",
      "Epoch 1980 loss: 3.357091821730137. Elbo loss: 0.11816568480383965. Generator loss: 0.444572881344826. Discriminator loss: 0.1111868437259428. Classification loss: 0.0019506790011937188. Regression loss: 0.006005546196378888\n",
      "Epoch 1990 loss: 3.403679572045803. Elbo loss: 0.11681731357689827. Generator loss: 0.44447078435651716. Discriminator loss: 0.11123729016511671. Classification loss: 0.006881136403066076. Regression loss: 0.00868417109802155\n"
     ]
    }
   ],
   "execution_count": 90
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
